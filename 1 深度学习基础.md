## 深度学习基础

##### 什么是深度学习

```
经典程序设计：规则 + 数据 = 答案
机器学习： 数据 + 答案 = 规则

机器学习的重要组成部分：数据集 数据集的表示方式也是找到理想中规则的重要的一部分 相同数据使用不同的表示方式有时候就可以将问题简化很多。

机器学习模型为输入数寻找合适的表示--对数据进行变换，使其更适合手头的任务

机器学习技术定义：在预先定义好的可能性空间中、利用反馈信号的指引来寻找输入数据的有用表示

深度学习
	数据模型中包含了多少层被称为模型的深度
	深度学习网络看作多极信息蒸馏操作：信息穿过连续的过滤器，其纯度越来也高
	深度学习对输入数据的具体操作保存在改成的权重，本质上是一串数字，权重又被称为权重。
	
神经网络损失函数，又被称为目标函数，是用来衡量该输出与预期值之间的距离的一个函数。
	预测值 + 真实目标值 通过损失函数  ---》 损失值

深度学习利用损失函数的距离值作为反馈信号来对权重值进行微调，降低当前示例的损失，优化器可以进行这种调节，实现了所谓的反向传播（backpropagation）算法

训练循环：一开始对权重随机赋值，参考损失值的情况向正确的方向进行微调，随着示例增多，损失值逐渐降低。

核方法，比较比较典型的方法就是SVM支持向量机
	SVM的目标是通过在属于两个不同类别的两组数据点之间找到一个良好决策边界，完成分类问题。
		SVM寻找边界（1）将数据映射到一个新的高维，这是决策的边界可以用一个超平面表示。（2）尽量让超平面与每个类别最近的数据点之间最大化，叫做间隔最大化。
```

###### 神经网络工作机制

![神经网络机制](C:\Users\lenovo_zhuang\Desktop\学习笔记MD\阅读笔记\img\神经网络机制.jpg)

***形式推理，协助人类进行软件开发和科学研究活动***

##### 决策树 随机森林与梯度提升机

##### 



##### 神经网络的数学基础

###### 神经网络架构

```java
核心组件： 层（layer） 类似于数据过滤器，简单的层连接起来实现渐进式数据蒸馏
训练网络需要编译步骤的三个参数
    损失函数（loss function）:网络衡量在训练数据上的性能，使网络朝着正确的方向前进
    优化器（optimizer）: 基于训练数据和损失函数来更新网络的机制
    在训练和测试过程中需要监控的指标（metric）: 正确分类的图像所占的比例
```

###### 编译步骤

```
数据预处理，变换为网络要求的形状，并且缩放所有值在[0,1]的区间内
准备图像数据
准备标签
	训练过程中出现两个标签 loss 损失 acc 精度

过拟合：机器学习模型在新的数据上的性能往往低于训练模型
```

###### 神经网络的数据表示

```
多维数组中存储的数据叫做张量，张量的维度叫做轴
维度： 表示沿着某轴上的元素个数，也可以表示张量中轴的个数
1.标量（0D张量）scalar
	仅包含一个数字的张量叫做标量(标量张量，零维张量，0D张量)，可以用ndim属性查看一个numpy的标量轴的个数，张量轴的个数又叫做阶。
2.向量（1D张量）vector
	数字组成的数组叫做向量，一维张量只有一个轴。假定向量有n个元素，那就是nD向量。
3.矩阵（2D张量）matrix
	向量组成的数组叫做矩阵，或者二维张量。矩阵有2个轴。
	
关键属性
	1. 轴的个数（阶）：张量的ndim
	2. 形状： 整数元组，表示张量沿每个轴的维度大小
	3. 数据类型： python库中表示为dtype,这是张量中数据的类型
```



###### 在numpy中操作张量

```
张量切片： 选定张量中的特定元素
```

![](C:\Users\lenovo_zhuang\Desktop\学习笔记MD\阅读笔记\img\数组张量操作.png)

~~~
数据批量概念
	深度学习中所有数据张量的第一个轴就是样品轴，有时也叫作样品抽样，将数据拆分成小批量，就是一个数据批量
~~~

向量数据： 第一轴是样品轴，第二轴是特征轴

时间序列数据

图像数据： 三个维度 高度、宽度、颜色深度



###### “齿轮”张量运算

```python
output = relu(dot(w, input) + b)
dot是两个张量做点积运算 
```

###### 逐元素运算

~~~
relu运算和加法运算都是逐元素运算，该运算独立地应用于张量的每个元素。
~~~

###### 广播

~~~
当两个形状不同的张量相加，较小的张量会被广播
主要步骤：
	1.像较小的张量添加轴（广播轴），使其ndim与较大的张量相同
	2.将较小的张量沿着新轴重复，使其形状与较大的长相相同
~~~

###### 张量点积

~~~
张量积，与逐元素乘积不同
数学符号中（.）表示点积运算
z = x.y  元素个数相同才能做点击，返回值是一个标量，相当于矩阵、向量的乘法
~~~

###### 张量变形

~~~
张量变形是改变张量的行和列，以得到想要的形状，进行数据预处理时用到，变形后张量的元素个数保持不变
经常使用的是转置
~~~

###### 神经网络的“引擎”：基于梯度的优化

~~~~
抽取训练样本x和对应目标y组成的数据批量
在x上运行网络（前向传播），得到预测值y_pred
计算网络在数据上的损失
更新网络权重，使得网络上的这批孙淑略微下降

网络中的所有运算都是可微的，计算机网络系数梯度，然后向梯度反方向改变系数，从而降低损失

随机梯度下降，动量改变学习的步长，运用物理学灵感，参考之前速度和加速度。
~~~~

###### 反向传播算法

~~~
利用链式求导应用于神经网络中梯度值的计算机，得到的算法就是反向传播。
~~~

##### 神经网络入门

###### 层

~~~
神经网络的基本结构，深度学的基础组件
不用张量格式和数据类型使用不同的层，通常
	2D张量 密集连接层，全连接层
	3D张量 循环层
	4D张量 二维卷积层

层兼容性： 每一层只能接受特定形状的输入张量，并返回特定形状的输出张量。
~~~

###### 模型

~~~
模型：层构成的网络，有向无环图
~~~

###### 损失函数和优化器

~~~
配置学习过程的关键

损失函数（目标函数）：在训练过程中需要将其最小化
优化器： 决定如何基于损失函数对网络进行更新
~~~

###### 使用keras开发

~~~
1. 定义训练数据
2. 定义层组成网络，将输入映射到目标
3. 配置学习过程
4. 调用模型的fit方法在训练集上进行迭代
~~~

###### 二分类问题

~~~
1.加载数据集 IMDB电影评论数据集

~~~

